apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ksm-alerts
  namespace: {{ index .Params "Namespace" }}
spec:
  groups:
    - name: general.rules
      rules:
      - alert: KubePodCrashLooping
        annotations:
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts_and_troubleshooting.md
          message: Pod {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} ({{ "{{" }} $labels.container {{ "}}" }}) is restarting {{ "{{" }} printf "%.2f" $value {{ "}}" }} times every 5 minutes; for the last 15 minutes.
        expr: |
          rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) * on (namespace, namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"} * 60 * 5 > 0
        for: 15m
        labels:
          severity: critical
      - alert: KubePodNotReady
        annotations:
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts_and_troubleshooting.md
          message: Pod {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} has been in a non-ready state for longer than 15 minutes.
        expr: |
          sum by(pod, namespace) (kube_pod_status_phase{phase=~"Pending|Unknown"} * on (namespace, namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"}) > 0
        for: 15m
        labels:
          severity: critical
      - alert: KubePodImagePullBackOff
        annotations:
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts_and_troubleshooting.md
          message: Pod {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} has been unable to pull it's image for longer than 5 minutes.
        expr: |
          (kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"} * on (namespace, namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"}) > 0
        for: 5m
        labels:
          severity: critical
      - alert: KubePodBadConfig
        annotations:
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts_and_troubleshooting.md
          message: Pod {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} has been unable to start due to a bad configuration for longer than 5 minutes.
        expr: |
          (kube_pod_container_status_waiting_reason{reason="CreateContainerConfigError"} * on (namespace, namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"}) > 0
        for: 5m
        labels:
          severity: critical
      - alert: KubePodStuckCreating
        annotations:
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts_and_troubleshooting.md
          message: Pod {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} has been trying to start for longer than 15 minutes - this could indicate a configuration error.
        expr: |
          (kube_pod_container_status_waiting_reason{reason="ContainerCreating"} * on (namespace, namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"}) > 0
        for: 15m
        labels:
          severity: critical
      - alert: ClusterSchedulableMemoryLow
        annotations:
          message: The cluster has {{ "{{" }} printf "%.0f" $value {{ "}}" }}% of memory requested and unavailable for scheduling for longer than 15 minutes.
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts/Cluster_Schedulable_Resources_Low.asciidoc
        expr: |
           ((sum(sum by(node) (sum by(pod, node) (kube_pod_container_resource_requests_memory_bytes * on(node) group_left() (sum by(node) (kube_node_labels{label_node_role_kubernetes_io_compute="true"} == 1))) * on(pod) group_left() (sum by(pod) (kube_pod_status_phase{phase="Running"}) == 1)))) / ((sum((kube_node_labels{label_node_role_kubernetes_io_compute="true"} == 1) * on(node) group_left() (sum by(node) (kube_node_status_allocatable_memory_bytes)))))) * 100 > 85
        for: 15m
        labels:
          severity: warning
      - alert: ClusterSchedulableCPULow
        annotations:
          message: The cluster has {{ "{{" }} printf "%.0f" $value {{ "}}" }}% of CPU cores requested and unavailable for scheduling for longer than 15 minutes.
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts/Cluster_Schedulable_Resources_Low.asciidoc
        expr: |
           ((sum(sum by(node) (sum by(pod, node) (kube_pod_container_resource_requests_cpu_cores * on(node) group_left() (sum by(node) (kube_node_labels{label_node_role_kubernetes_io_compute="true"} == 1))) * on(pod) group_left() (sum by(pod) (kube_pod_status_phase{phase="Running"}) == 1)))) / ((sum((kube_node_labels{label_node_role_kubernetes_io_compute="true"} == 1) * on(node) group_left() (sum by(node) (kube_node_status_allocatable_cpu_cores)))))) * 100 > 85
        for: 15m
        labels:
          severity: warning
      - alert: PVCStorageAvailable
        annotations:
          message: The {{ "{{" }} $labels.persistentvolumeclaim {{ "}}" }} PVC has has been {{ "{{" }} printf "%.0f" $value {{ "}}" }}% full for longer than 15 minutes.
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts/Cluster_Schedulable_Resources_Low.asciidoc
        expr: |
          ((sum by(persistentvolumeclaim, namespace) (kubelet_volume_stats_used_bytes) * on ( namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"}) / (sum by(persistentvolumeclaim, namespace) (kube_persistentvolumeclaim_resource_requests_storage_bytes) * on ( namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"})) * 100 > 85
        for: 15m
        labels:
          severity: warning
      - alert: PVCStorageMetricsAvailable
        annotations:
          message: PVC storage metrics are not available
          sop_url: https://github.com/RHCloudServices/integreatly-help/blob/master/sops/alerts/Cluster_Schedulable_Resources_Low.asciidoc
        expr: |
          absent(kubelet_volume_stats_available_bytes) == 1 or absent(kubelet_volume_stats_capacity_bytes) == 1 or absent(kubelet_volume_stats_used_bytes) == 1 or absent(kube_persistentvolumeclaim_resource_requests_storage_bytes) == 1
        for: 15m
        labels:
          severity: warning
      - alert: PVCStorageWillFillIn4Days
        annotations:
          message: The {{ "{{" }} $labels.persistentvolumeclaim {{ "}}" }} PVC will run of disk space in the next 4 days.
        expr: |
          (
            predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) <= 0
              AND
            kubelet_volume_stats_available_bytes{job="kubelet"} / kubelet_volume_stats_capacity_bytes{job="kubelet"} < 0.25
          )
          * on(namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"}
        for: 15m
        labels:
          severity: warning
      - alert: PVCStorageWillFillIn4Hours
        annotations:
          message: The {{ "{{" }} $labels.persistentvolumeclaim {{ "}}" }} PVC will run of disk space in the next 4 hours.
        expr: |
          (
            predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[1h], 4 * 3600) <= 0
              AND
            kubelet_volume_stats_available_bytes{job="kubelet"} / kubelet_volume_stats_capacity_bytes{job="kubelet"} < 0.25
          )
          * on(namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"}
        for: 15m
        labels:
          severity: critical
      - alert: PersistentVolumeErrors
        annotations:
          message: The PVC {{ "{{" }} $labels.persistentvolumeclaim {{ "}}" }} is in status {{ "{{" }} $labels.phase {{ "}}" }} in namespace {{ "{{" }} $labels.namespace {{ "}}" }}
        expr: |
          (sum by(persistentvolumeclaim, namespace, phase) (kube_persistentvolumeclaim_status_phase{phase=~"Failed|Pending|Lost"}) * on ( namespace) group_left(label_monitoring_key) kube_namespace_labels{label_monitoring_key="middleware"}) > 0
        for: 15m
        labels:
          severity: critical
